#!/usr/bin/perl
########################################################################
#  Script Name : api-post-image
#  Written by  : Olivier Lahaye
#  Date        : August 04, 2015
#  Purpose     : This post_install script deploys the slurm.conf into images
#  Copyright (c) Olivier Lahaye 2015
#                All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
#
#########################################################################
# $Id: api-post-clientdef 10570 2015-08-04 14:58:43Z olahaye74 $
#########################################################################

use OSCAR::Opkg;
use OSCAR::Package;
use OSCAR::Utils;
use OSCAR::Logger;
use OSCAR::LoggerDefs;
use OSCAR::SystemServices;
use OSCAR::SystemServicesDefs;
use OSCAR::OCA::OS_Settings;
use OSCAR::FileUtils;
use SystemInstaller::Machine; # get_machine_listing()
#use Getopt::Long;  # Used to get command line option
use strict;
use Carp;
use v5.10.1;
# Avoid smartmatch warnings when using given
no if $] >= 5.017011, warnings => 'experimental::smartmatch';

# Names w/o numeric suffix preceed those with numeric suffix.
sub sortnodes(@) {
	return map { $_->[0] }
	       sort { $a->[1] cmp $b->[1] || ($a->[2]||-1) <=> ($b->[2]||-1) }
	       map { [$_, /^([\D]+)([\d]*)$/] }
	       @_;
}

# Get all configured packages so we can check the one we may interract with (jobmonarch, blcr, maui, openmpi, ...)
my @pkgs = OSCAR::Database::list_selected_packages();

# OSCAR Default values for slurm.conf.
my $cluster_name="OSCAR Cluster";
my $control_machine="oscar-server";
my $auth_type="auth/munge";
my $checkpoint_type="checkpoint/blcr";
my $crypto_type="crypto/munge";
my $compute_on_head = "NO"; 

# Read in configurator values for slurm
my $configvalues = getConfigurationValues('slurm');

# User has configured slurm package.
if (defined ($configvalues)) {
    $cluster_name = $configvalues->{cluster_name}[0];
    $control_machine = $configvalues->{control_machine}[0];
    $auth_type = $configvalues->{auth_type}[0];
    $checkpoint_type = $configvalues->{checkpoint_type}[0];
    $crypto_type = $configvalues->{crypto_type}[0];
    $compute_on_head = $configvalues->{compute_on_head}[0];
}

# Get the config file name and back it up is needed.
my $config_file = OSCAR::OCA::OS_Settings::getitem(SLURM()."_configfile");
if (-e $config_file) {
    # 1st, create a backup of the config file if not already done.
    backup_file_if_not_exist($config_file) or return -1;
}

# Open file for writing.
open(CONF,">$config_file") or (carp "Couldn't open $config_file for writing!",
                         and return undef); # FIXME: check what to return.

print CONF <<EOF;
# slurm.conf file generated by $0.
# Please run configurator.html (in doc/html) to build a configuration file customized
# for your environment. Copy this file on all nodes.
# See the slurm.conf man page for more information.
#
ClusterName=$cluster_name
ControlMachine=$control_machine
SlurmUser=slurm
SlurmctldPort=6817
SlurmdPort=6818
AuthType=$auth_type
CacheGroups=1
CheckpointType=$checkpoint_type
CryptoType=$crypto_type
CheckpointType=$checkpoint_type
#EnforcePartLimits=YES
PrivateData=jobs
ProctrackType=proctrack/pgid
ReturnToService=1
StateSaveLocation=/var/spool
SlurmdSpoolDir=/var/spool/slurmd
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid

EOF

if ( 'jobmonarch' ~~ @pkgs ) {
    # TODO: jobmonarch
# If jobmonarch enabled need accounting infos
# AccountingStorageEnforce=limits
# AccountingStorageHost=$control_machine
# AccountingStoragePort=6819
# AccountingStorageType=accounting_storage/slurmdbd
# AccountingStoreJobComment=YES

    opkg_print("Accounting not yet supported in slurm config (jobmonarch)\n");
# Update the server's nodes file (optionally for only one image).
}

if( 'blcr' ~~ @pkgs ) {
    # TODO: blcr
    opkg_print("Make sure you've set CheckpointType= to checkpoint/blcr\n");
}

#my $image = shift; # Optionally provide an image name.

########################
#  BEGIN MAIN PROGRAM  #
########################

oscar_log(5, INFO, "Compute on head node is set to $compute_on_head\n");

# Check if we're using MAUI Scheduler
if ( 'maui' ~~ @pkgs ) {
    # Using MAUI: need to add some slurm config
    opkg_print("Using MAUI with slurm (not tessted/supported)\n");
    print CONF <<EOF;
#
# MAUI specific part
SchedulerType=sched/wiki
SchedulerPort=7321
# Check that AuthKey is correctly set up in wiki.conf
# See https://computing.llnl.gov/linux/slurm/maui.html for more info
# End of MAUI specific part
EOF
    oscar_log(5, WARNING, "wiki.conf file not created. More info here: https://computing.llnl.gov/linux/slurm/maui.html");
}

opkg_print("Updating slurm nodes\n");

# Get listing of nodes from the SIS database
my %nodes = get_machine_listing();

# OL: TODO: Code duplication. Need to use nodes code
if ($compute_on_head eq "YES") {
  # Get server proc count
  my $server_procs = 0;
  open(CPUINFO, "/proc/cpuinfo");
    my @lines = <CPUINFO>;
  close(CPUINFO);
  foreach my $line ( @lines ) {
    chomp($line);
    if ($line =~ /^processor\s*:.*$/){
       $server_procs++
    }
  }

  # Add server to node list hash
  my %added_node ;
  $added_node{HOST}=$ENV{HOSTNAME};
  $added_node{IPADDR}="";
  $added_node{NUM_PROCS}=$server_procs;
  $added_node{NUM_GPUS}=0; # Ignore GPUS on head (too much dangerous)
  $added_node{DOMAIN}="";
  $nodes{$ENV{HOSTNAME}}=\%added_node;
}
my $TOT_NODES = 0;
my $TOT_NP = 0;
my $hostname = "";
my $all_nodes = "";

print CONF "\n#\n# Node Configurations\n#\n";

# Add nodes to slurm config file
foreach my $node (sortnodes( keys %nodes )) {
    # If NUM_PROCS is not defined for this node, skip this node entirely
    next unless ($nodes{$node}{NUM_PROCS});

    # Check each node against the output from 'slurmnodes -a'
    $hostname = $nodes{$node}{HOST};
    print CONF "NodeName=".$nodes{$node}{HOST}." CPUs=".$nodes{$node}{NUM_PROCS}." State=UNKNOWN\n"; # GPUs: $nodes{$node}{NUM_GPUS}
    $all_nodes .= $nodes{$node}{HOST}." ";

    # Count up the number of nodes and processors
    $TOT_NODES++;
    $TOT_NP += $nodes{$node}{NUM_PROCS};
}

# Next, set up the values for workq
opkg_print("Creating SLURM workq partition...\n");

print CONF "\n#\n# Partition Configurations\n#";
print CONF "PartitionName=workq $all_nodes Default=YES MaxTime=INFINITE State=UP\n";
 
close(CONF);

# copy client config to images
opkg_print "Copying slurm configuration file to images:\n";
my @images = list_image();
foreach my $image (@images) {
    my $dir = $image->location;
    opkg_print " -> $dir\n";
    copy($config_file,"$dir$config_file") or (carp "Can't copy $config_file to $dir",
						    return undef);
}

# We need to deploy config file on nodes
opkg_print "Copying slurm configuration file to nodes:\n";
oscar_system("/usr/bin/cpush $config_file");


# We need to start the slurm daemon here as it needs to be up when deploying nodes.
# Needs to be listenning when node boots.
oscar_log(5, INFO, "Starting slurm daemon");
!system_service(SLURM,START)
    or (oscar_log(5, ERROR, "Couldn't start slurm service."), return -1);

# Restart slurm on nodes.
!remote_system_service(SLURM,RESTART,"/usr/bin/cexec")
    or (oscar_log(5, ERROR, "Couldn't restart slurm on all nodes."), exit 1);
