% -*- latex -*-
%
% $Id: detailed.tex,v 1.3 2001/12/16 19:32:00 mchasal Exp $
%
% $COPYRIGHT$
%

\section{Detailed Cluster Installation Procedure}

Note: All actions specified herein should be performed by the
\user{root} user.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Server Installation and Configuration}
  
During this phase, you will prepare the machine to be used as the
server for using OSCAR.

\subsubsection{Install Linux on the server machine} 

If you have a machine you want to use that already has Linux
installed, you may use it and continue with the next section. When installing
Linux, it is required that you use a distribution that is based upon
the RPM standard.  Furthermore, it should be noted that all testing up
to this point has been done using the Red Hat 7.1 distribution. As
such, use of distributions other than Red Hat 7.1 will require a
porting of OSCAR, as many of the scripts and software within OSCAR are
dependent on Red Hat. Do not worry about doing a custom install, as
OSCAR contains all the software on which it depends.  The only other
installation requirement is that some X environment such as GNOME or
KDE must be installed. Therefore, a typical workstation install is
sufficient.

\begchange
If you install Red Hat 7.1 on the server machine, during the
installation you should enable the ipchains-base firewall that is 
included with the Red Hat distribution in medium mode.
Other firewalls that are stronger and more versatile can be installed
later, but this will offer some protection until that time.
Note that OSCAR currently assumes that only the server machine is
exposed to the general network, with the server and the rest of the
cluster's machines being on a private network. To keep the Red Hat
firewall from interfering with network traffic between the server
machine and the other machines in the cluster, OSCAR automatically
disables portions of the Red Hat firewall. This may not have the
intended results in the situation where all the cluster machines
are exposed on the general network.
See Appendix~\ref{app:security} for more information about firewalls
and other security software that can be installed. 
\endchange

\subsubsection{Disk space and directory considerations}

OSCAR has certain requirements for server disk space. Space will be
needed to store the Linux rpms and to store the images.
The rpms will be stored in /tftpboot/rpm. Approximately 1 gigabyte is required
to store the rpms. 
The images are stored in /var/lib/systemimager and will need approximately
1 gigabyte per image. Only 1 image is required, although you may want to create
more in the future. 

If you are installing a new server, it is suggested that you allow for 
2 gigabytes in both the \file{/}, which contains \file{/tftpboot},  
and \file{/var} filesystems when partitioning the disk on your server.

If you are using an existing server, you will need to verify that you have 
enough space on the disk partitions. Again 2 gigabytes of free space
is recommended in both the \file{/} and \file{/var} partitions.

You can check
the amount of free space on your drive's partitions by issuing the
command \cmd{df -h} in a terminal.  The result for each file system is
located below the \panel{Avail} heading. If your root (\file{/})
partition has enough free space, enter the following command in a
terminal:

\begin{verbatim}
  mkdir -p /tftpboot/rpm
\end{verbatim}
  
If your root partition does not have enough free space, create the
directories on a different partition that does have enough free space
and create links to them from the root (\file{/}) directory.  For
example, if the partition containing \file{/usr} contains enough
space, you could do so by using the following commands:

\begin{verbatim}
  mkdir -p /usr/tftpboot/rpm
  ln -s /usr/tftpboot /tftpboot
\end{verbatim}

The same procedure should be repeated for the \file{/var/lib/systemimager}
subdirectory.

    
\subsubsection{Get a copy of OSCAR and unpack on the server} 

If you are reading this, you probably already have a copy. If not, go
to \url{http://oscar.sourceforge.net/} and download the latest OSCAR
tarball, which will be named something like \file{ oscar-version.tgz}.
The version used in these instructions is \oscarversion, which you
should replace with the version you download in any of the sample
\begchange
commands. Copy the OSCAR tarball to a directory such as \file{/root} on
\endchange
your server. There is no required installation directory, except that
you may not use \file{/usr/local/oscar}, which is reserved for special
use. Do {\bf not} unpack the tarball on a Windows based machine and
copy the directories over to the server, as this will convert all the
scripts to the dreaded ``DOS'' format and will render them useless
under Linux.  Assuming you placed the OSCAR tarball in
\begchange
\file{/root},
\endchange
open a command terminal and issue the following commands to unpack
OSCAR:

\begchange
\begin{verbatim}
  cd /root
  tar -zxvf oscar-<VERSION>.tgz
\end{verbatim}
\endchange
    
The result is the creation of an OSCAR directory structure that is
laid out as show in Table~\ref{tab:oscar-dir-struct} (again assuming
\begchange
\file{/root}).
\endchange

\begchange
\begin{table}[htbp]
  \begin{center}
    \begin{tabular}{|l|p{3in}|}
      \hline
      \multicolumn{1}{|c|}{Directory} &
      \multicolumn{1}{|c|}{Contents} \\
      \hline
      \hline
      \file{/root/OSCAR-\oscarversion/} & the base OSCAR directory \\
%
      \file{/root/OSCAR-\oscarversion/COPYING} & GNU General Public License
      v2 \\
%
      \file{/root/OSCAR-\oscarversion/README.first} & README first document \\
%
      \file{/root/OSCAR-\oscarversion/c3} & contains files for C3
      installation \\
%
      \file{/root/OSCAR-\oscarversion/docs} & OSCAR documentation directory \\
%
      \file{/root/OSCAR-\oscarversion/install\_cluster} & main installation
      script \\
%
      \file{/root/OSCAR-\oscarversion/lui} & contains files for LUI
      installation \\
%
      \file{/root/OSCAR-\oscarversion/oscarResources} & contains sample OSCAR
      resources \\
%
      \file{/root/OSCAR-\oscarversion/oscarRPM} & contains RPMs for software
      installed \\
%
      \file{/root/OSCAR-\oscarversion/pbs} & contains files for PBS
      installation \\
%
      \file{/root/OSCAR-\oscarversion/prog\_env} & contains files for MPI and
      PVM installations \\
%
      \file{/root/OSCAR-\oscarversion/scripts} & contains scripts that do most
      of the work \\
%
      \file{/root/OSCAR-\oscarversion/systemimager} & contains files for
      SystemImager installation \\
%
      \file{/root/OSCAR-\oscarversion/testing} & contains OSCAR Cluster Test
      software \\
      \hline
    \end{tabular}
    \caption{OSCAR file directory layout.}
    \label{tab:oscar-dir-struct}
  \end{center}
\end{table}
\endchange
  
\subsubsection{Configure the ethernet adapter for the cluster} 

Assuming you want your server to be connected to both an external
network and the internal cluster subnet, you will need to have two
ethernet adapters installed in the server. It is preferred that you do
this because exposing your cluster may be a security risk, and certain
software used in OSCAR such as DHCP may conflict with your external
network.  Once both adapters have been physically installed and you
have booted Linux into an X environment, open a terminal and enter the
command:

\begin{verbatim}
  /usr/sbin/netcfg &
\end{verbatim}
  
The network configuration utility will be started, which you will use
to configure your network adapters.
  
At this point, the \panel{Names} panel will be active. On this panel
you will find the settings for the server's hostname, domain,
additional search domains, and name servers. All of this information
should have been filled in by the standard Linux installation. To
configure your ethernet adapters, you will need to first press the
\button{Interfaces} button to bring up the panel that allows you to
update the configuration of all of your server machines interfaces.
You should now select the interface that is connected to the cluster
network by clicking on the appropriate device. If your external
adapter is configured on device ``\file{eth0}'', then you should most
likely select ``\file{eth1}'' as the device, assuming you have no
other adapters installed. After selecting the appropriate interface,
press the \button{Edit} button to update the information for the
cluster network adapter. Enter a private IP address
\begchange
\footnote
{
  There are
  three private IP address ranges: 10.0.0.0 to 10.255.255.255;
  172.16.0.0 to 172.32.255.255; and 192.168.0.0 to 192.168.255.255.
  Additional information on private intranets is available in RFC
  1918.
  You should not use the IP addresses 10.0.0.0 or 172.16.0.0 or 
  192.168.0.0 for the server.  If you use one of these addresses 
  the network installs of the client nodes will fail (rpc has 
  problems).
}
\endchange
and the associated netmask\footnote{The netmask
  255.255.255.0 should be sufficient for most OSCAR clusters.}  in
their respective fields. Additionally, you should be sure to press the
\button{Activate interface at boot time} button and set the
\button{Interface configuration protocol} to ``none''.  After
completing the updates, press the \button{Done} button to return to
the main utility window,
\begchange
pressing the \button{Save} button in the Save current configuration
menu that pops up.
\endchange
Then press the \button{Save} button at the bottom of the main network
configuration window to confirm your changes, and then press the
\button{Quit} to leave the network configuration utility.
  
Now reboot your machine to ensure that all the changes are propagated
to the appropriate configuration files. To confirm that all ethernet
adapters are in the \msg{UP} state, once the machine has rebooted,
open another terminal window and enter the following command:

\begin{verbatim}
  /sbin/ifconfig -a
\end{verbatim}
  
You should see \msg{UP} as the first word on the third line of output
for each adapter. If not, there is a problem that you need to resolve
before continuing. Typically, the problem is that the wrong module is
specified for the given device. Try using the network configuration
utility again to resolve the problem.
  
\subsubsection{Copy distribution RPMs to \file{/tftpboot/rpm}}

In this step, you need to copy the RPMs included with your Linux
distribution into the \file{/tftpboot/rpm} directory. 
\begchange
Insert each of the distribution CDs in turn.
When each one is inserted, linux will automatically make the
contents of the CD be available in the \file{/mnt/cdrom} directory.
Then for each CD locate the directory that contains the RPMs.
In Red Hat 7.1, the RPMs are located in the \file{RedHat/RPMS}
directory, which will appear on the system as the 
\file{/mnt/cdrom/RedHat/RPMS} directory. 
\endchange
 After locating the RPMs on the each CD, copy them into
\file{/tftpboot/rpm} with a command such as:

\begin{verbatim}
  cp /mnt/cdrom/RedHat/RPMS/*.rpm /tftpboot/rpm
\end{verbatim}
  
Be sure to repeat the above process for both CDs when using Red Hat
7.1.
\begchange
After using each CD you will have to unmount it from the local
file system by issuing these commands:

\begin{verbatim}
  cd
  umount /mnt/cdrom
\end{verbatim}
\endchange
If you wish to save space on your server's hard drive and will be
using the default RPM list supplied with OSCAR (see
Section~\ref{sec:detailed-cluster-def},
item~\ref{sec:detailed-define-rpm-list} on
page~\pageref{sec:detailed-define-rpm-list} for more information on
this), you should only copy over the RPMs listed in the sample.
\begchange
For the Red Hat 7.1 distribution, this can be done for each CD with
a command sequence like this:

\begin{verbatim}
  cd /mnt/cdrom/RedHat/RPMS
  cat /root/OSCAR-\oscarversion/oscarsamples/sample.rpmlist | \
  xargs -i sh -c "cd `pwd`;ls -1 {}*.rpm" 2>/dev/null | \
  xargs -i -t cp -f '{}' /tftpboot/rpm
\end{verbatim}

\endchange

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\subsection{Initial OSCAR Server Configuration}

During this phase, the software needed to run OSCAR will be installed
on the server. In addition, some initial server configuration will be
performed. The steps from here forward should be run within the X
environment, due to the graphical nature of the OSCAR.

\subsubsection{Change to the OSCAR directory and run \file{install\_cluster}}

\begchange
If the OSCAR directory was placed in \file{/root} for example, you
would issue the following commands:

\begin{verbatim}
  cd /root/OSCAR-\oscarversion
  ./install_cluster eth0
\end{verbatim}
\endchange
  
\begchange
In the above command, substitute the device name 
\endchange
(e.g., \file{eth0})
for your server's internal ethernet adapter. Also note that the
\file{install\_cluster} script must be run from within the OSCAR base
directory as shown above. The script will first run the part one
server configuration script, which does the following:

\begin{enumerate}
\item copies Oscar rpms to /tftpboot/rpm
\item installs all server Oscar rpms
\item updates \file{/etc/hosts} with OSCAR aliases
\item updates \file{/etc/exports} 
\item adds Oscar paths to \file{/etc/profile} 
\item updates system startup (\file{/etc/rc.d/init.d}) scripts
\item restarts affected services
\end{enumerate}
  
If the part one script finishes successfully, \file{install\_cluster}
will then start the OSCAR wizard. The wizard, as shown in
Figure~\ref{fig:detailed-oscar-wizard}, is provided to guide you
through the rest of the cluster installation.  To use the wizard, you
will complete a series of steps, with each step being initiated by the
pressing of a button on the wizard. Do not go on to the next step
until the instructions say to do so, as there are times when you must
complete an action outside of the wizard before continuing on with the
next step. For each step, there is also a \button{Help} button located
directly to the right of the step button. When pressed, the
\button{Help} button displays a message box describing the purpose of
the step.

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[scale=\imgscale]{oscar-wizard.\figext}
    \caption{OSCAR Wizard.}
    \label{fig:detailed-oscar-wizard}
  \end{center}
\end{figure}
  
As each of the steps are performed, there is output generated that is
displayed to the user. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Cluster Definition}
\label{sec:detailed-cluster-def}

During this phase, you will complete steps one through five of the
OSCAR wizard in defining your cluster. If you encounter problems or
wish to redo any of the SIS actions performed in the wizard steps 1,
or 2, please refer to the SIS man pages.

\subsubsection{Define the image} 

Press the Step 1 button of the wizard entitled \button{Define the
  Server}. In the dialog box that is displayed, enter a name for the
LUI server machine. For consistency purposes, you should use the same
name as was used when configuring your internal ethernet adapter. Next
enter the internal server IP address and cluster netmask. The last
piece of information you need to provide is the default reboot action
for all clients associated with the server after they have
successfully completed their installation. You can either have them
automatically reboot by selecting ``true'', or show a prompt by
selecting ``false''. \msg{It is important to note that if you wish to
  use automatic reboot, you should make sure the BIOS on each client
  is set to boot from the local hard drive before attempting a network
  boot by default. If you have to change the boot order to do a
  network boot before a disk boot to install your client machines, you
  should not use automatic reboot.} Press the \button{Apply} button
when finished.  A sample dialog with input and successful output is
shown in Figure~\ref{fig:detailed-define-server}. If any warning
messages about the nfsd service and the exporting of the \file{/usr}
and \file{/tftpboot} directories are displayed in the output, you may
safely ignore them since these requirements are met later in the OSCAR
installation procedure. Press the \button{Close} button to complete
the step.

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[scale=\imgscale]{define-server.\figext}
    \caption{Define the server.}
    \label{fig:detailed-define-server}
  \end{center}
\end{figure}
  
\subsubsection{Collect client MAC addresses} 

What you do at this time is dependent upon whether you already know
the MAC addresses of your clients or not. The MAC address of a client
is a twelve hex-digit hardware address embedded in the client's
ethernet adapter. MAC addresses look like 00:0A:CC:01:02:03, as
opposed to the familiar format of IP addresses.

\begin{enumerate}
\item If you do know the addresses, you should edit the file
  \file{/etc/MAC.info} using your favorite editor. The format for the
  file is one line for each client, where each line is of the form:

\begin{verbatim}
  node-id  MAC-address
\end{verbatim}
    
  The node id is simply a symbolic tag used by different collection
  tools to refer to the MAC address. Typically, one would use a node
  id such as ``\file{node0}''. Enter a node id and MAC address for
  each of your clients. Be sure to write down the node id for the
  first client you enter, which will be used later in defining the
  client machines.
    
\item If you do not know the client MAC addresses, or do not want to
  edit the \file{/etc/MAC.info} file manually, press the Step 2 button
  of the wizard entitled \button{Collect Client MAC Addresses}. The
  OSCAR MAC address collection utility dialog box will be displayed.
  To start the collection, press the \button{Collect} button and then
  network boot the first client.  Follow the directions given in the
  dialog output window to collect the rest of your clients' addresses.
  Please do not press \button{Collect} more than needed, as each time
  it is pressed the utility will wait until a valid MAC is collected
  before continuing. In order to use this tool, you will need to know
  how to network boot your client nodes.  For instructions on doing
  so, see Appendix~\ref{app:net-boot-client-nodes}. A sample dialog
  that has been used to collect four client MAC addresses is shown in
  Figure~\ref{fig:detailed-collect-mac}. When you have collected the
  addresses for all your client nodes, press \button{Done}.

\end{enumerate}

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[scale=\imgscale]{collect-mac.\figext}
    \caption{Collect client MAC addresses.}
    \label{fig:detailed-collect-mac}
  \end{center}
\end{figure}
  
\subsubsection{Define your client machines} 

Press the Step 3 button of the wizard entitled \button{Define the
  Client Machines}. In the dialog box that is displayed, enter the
appropriate information.

\begin{enumerate}
\item In the \field{Starting IP Address} field, enter the first client
  IP entered in \file{/etc/hosts} in the previous step. The wizard
  will assume that the IP entered in this field should be associated
  with the starting MAC id entered later, or the first id in
  \file{/etc/MAC.info} if no id is specified.
    
\item For \field{Netmask}, enter the cluster netmask used when
  defining the server.
    
\item If you decided to edit the \file{/etc/MAC.info} file yourself,
  enter the node id of the first client in the \field{Starting MAC ID}
  field. Otherwise, you may leave this field blank.
    
\item Enter the number of client machines to create in \field{Number
    of clients to create}. The number of clients should be equal to
  the number of entries you added in \file{/etc/hosts}. If you leave
  this field blank, the number of clients created will default to the
  number of entries in \file{/etc/MAC.info} starting from the MAC id
  specified in the previous step. For example, if the starting MAC id
  was the fourth from the last entry in \file{/etc/MAC.info}, four
  clients will be created. If no starting MAC id was given, the number
  of clients created will be equal to the number of entries in
  \file{/etc/MAC.info}.
    
\item For \field{Default Route and Default Gateway}, enter the
  internal server IP address if you plan to use your server to route
  messages from your client nodes to the external network. Although
  OSCAR does not configure the server for routing of client messages,
  by entering the server IP now you can save yourself from having to
  manually update the clients later. If you have no plans to route
  internal messages to the external network, you may leave both fields
  blank.
    
\item In the \field{PBS String} field, you may enter an arbitrary
  string of words to describe your cluster nodes. The words in this
  string can be used within PBS to make scheduling decisions. When all
  of your client machines will be homogeneous, you can safely leave
  this field blank. However, if you plan to do multiple cluster
  installations using the same server, you should enter a string to
  distinguish these client nodes from those you may define and install
  at a later time. A suggested string in this case is the client group
  name specified in the next field.
    
\item In the \field{Machine Groups} field, enter a name for the group
  of clients to be created (e.g., ``myclients''). Note that this is
  \msg{not} optional, even though the wizard implies that it is.
  \msg{Remember to write down your client group name, as it will be
    used later}.
    
\item Finally, choose whether or not the clients should use the
  default server setting for automatically rebooting after
  installation. If they should, select ``\file{default}'' from the
  \field{Client auto-reboot} list. If not, then select the desired
  action by choosing ``true'' or ``false''.
\end{enumerate}
  
When finished entering information, press the \button{Apply} button.
A sample dialog with input and output is shown in
Figure~\ref{fig:detailed-define-clients}. After viewing the output,
which may be delayed by several minutes, press the \button{Close}
button and continue with the next step.

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[scale=\imgscale]{define-clients.\figext}
    \caption{Define the Clients.}
    \label{fig:detailed-define-clients}
  \end{center}
\end{figure}
    
\subsubsection{Define your resources} 

If you meet the following criteria for a homogeneous cluster, no
resource definitions need to be made and you may skip steps 4 and 5 of
the wizard.

Criteria:

\begin{enumerate}
\item Server and client machines contain identical hardware.
  
\item Server's root (\file{/}) filesystem is on same disk type (IDE
  or SCSI) as clients will use for their root filesystem.
    
\item Server is running the same kernel version as will be installed
  on the clients (must be RPM installed kernel). Note that UP and
  SMP kernels are considered different.
  
\item The default RPM list supplied by OSCAR
  (\file{OSCAR-\oscarversion/oscarResources/sample.rpmlist}) is
  acceptable for your clients.
\end{enumerate}

If you meet the above criteria and do not wish to specify any
resources in addition to or other than the defaults, you may skip
ahead to Section~\ref{sec:detailed-secondary-oscar-server-config},
Secondary OSCAR Server Configuration. Otherwise, press the Step 4
button of the wizard entitled \button{Define the Resources}.  The
``Define a Resource'' dialog will be displayed, which is used to
define all the resources to be allocated to the clients. When defining
your resources, it is best to assign them to a resource group as you
go, otherwise you will need to know the names of all your resources
when allocating them to the client group defined in the previous step.
The ``Define a Resource'' dialog contains a \field{Resource Groups}
entry field in which you can enter the name of the resource group
(e.g., ``myresources'') to add resources to.  For each of the
resources defined below, be sure to enter your resource group name in
its field.  \msg{Remember to write down your resource group name, as
  it will be used later.} If you make any mistakes whilst defining
your resources, you will need to use LUI to delete the errant
resource. See the LUI documentation for instructions on deleting
resources.

\begin{enumerate}
\item \msg{Define a disk table.} Create a disk table that specifies
  how your client hard drives will be partitioned. Refer to the
  \file{/usr/local/lui/README.disk} file for instructions on creating
  a disk table. Additionally, OSCAR provides sample IDE and SCSI disk
  tables in the \file{oscarResources} subdirectory. Be sure to include
  \file{/home} in your disk table as an NFS partition. Each of the
  filesystems you include in the disk table will be automatically
  defined as file resources to LUI. If you do not define a disk
  resource, OSCAR will automatically define the disk resource using
  the sample disk table for the type of drive (IDE or SCSI) on which
  the server's root (\file{/}) filesystem resides. A sample disk table
  for an IDE drive with a 24MB boot partition, a 128MB swap partition,
  and the rest of the disk allocated to the root partition is shown
  below:

\begin{verbatim}
  /dev/hda1     ext2          24      m     y     /boot
  /dev/hda2     extended      *       m     n
  /dev/hda5     swap          128     m     
  /dev/hda6     ext2          *       m     n     /
  nfs           nfs           /home         rw    10.0.0.50
\end{verbatim}
    
  Once you have created your disk table, enter a name to refer to the
  disk resource by in the \field{Name} field, select ``disk'' in the
  resource type list, and enter the location of your disk table in the
  \field{Full Path Name} field. When finished, press the
  \button{Apply} button. A sample dialog showing a disk table being
  defined and the associated output is shown in
  Figure~\ref{fig:detailed-define-disk-resource}

  \begin{figure}[htbp]
    \begin{center}
      \includegraphics[scale=\imgscale]{define-disk.\figext}
      \caption{Define the disk resource.}
      \label{fig:detailed-define-disk-resource}
    \end{center}
  \end{figure}
    
\item \msg{Define a custom kernel (Optional).} \msg{If you would like
    to use the default kernel from your Linux distribution, do not
    define a kernel resource, as it will already be installed during
    the installation of RPMs.} If you have a compiled a custom kernel
  you would like to use on your client machines instead, you need to
  define a custom kernel resource for it by specifying a resource
  name, selecting ``kernel'' from the resource types, and entering the
  path to the custom kernel.  When finished, press the \button{Apply}
  button.
    
\item \msg{Define a system map (Optional).} \msg{If you did not define
    a custom kernel resource, you do not need to define a system map
    resource.} If you did define a custom kernel resource, you will
  need to have a system map file resource that is compatible with that
  kernel. Define the custom system map by entering a resource name,
  selecting ``map'' from the resource types, and entering the path to
  the map file. When finished, press the \button{Apply} button.
    
\item \msg{Define an initial ramdisk.} The initial ramdisk resource is
  one of the most important for a modular kernel, as it is crucial in
  making sure that clients boot after installation. See Appendix B for
  instructions on creating an initial ramdisk. If you do not create a
  ramdisk resource, OSCAR will automatically generate one using the
  information from the server. However, the initial ramdisk generated
  will only work if the hardware on your server and clients are
  identical and the kernel to be installed on the clients is the one
  currently running on the server. After creating the ramdisk, create
  a resource for it by entering a resource name, selecting ``ramdisk''
  from the resource types, and specifying the path to the ramdisk.
  When finished, press the \button{Apply} button. A sample dialog
  showing a ramdisk being defined and the associated output is shown
  in Figure~\ref{fig:detailed-define-initial-ramdisk-resource}.

  \begin{figure}[htbp]
    \begin{center}
      \includegraphics[scale=\imgscale]{define-ramdisk.\figext}
      \caption{Define the initial ramdisk resource.}
      \label{fig:detailed-define-initial-ramdisk-resource}
    \end{center}
  \end{figure}
    
\item \msg{Define the RPM list (Optional).} If you do not define an
  rpm resource, the sample OSCAR RPM list will be used automatically.
  OSCAR provides a sample RPM list that contains a minimal set of RPMs
  needed to install RedHat 7.1 and the cluster software on a client
  machine that is based on the i686 architecture. The list is located
  in the \file{oscarResources} subdirectory and is named
  \file{sample.rpmlist}. If you are using another Linux distribution
  or client machine architecture, you will need to create an RPM list
  that will work for that distribution and includes the RPMs located
  in the \file{oscarRPM} subdirectory. If you have additional RPMs for
  software that you would like installed on the clients, add them to
  the sample list and copy the RPMs into \file{/tftpboot/rpm}. Create
  the RPM list resource by specifying a resource name, selecting
  ``rpm'' from the resource types, and entering the location of the
  RPM list. When finished, press the \button{Apply} button.
    \label{sec:detailed-define-rpm-list}
  \end{enumerate}
  
\subsubsection{Allocate the resources to clients}

Press the Step 5 button of the wizard entitled \button{Allocate the
  Resources}. At this point you will need to know your client and
resource group names. If you did not specify a resource group when
defining your resources, see the paragraph below. Allocate the
resources to the clients by entering the resource group name in the
\field{Resource Groups} field and the client group name in the
\field{Machine Groups} field.  When finished, press the \button{Apply}
button. A sample dialog with input and output is shown in
Figure~\ref{fig:detailed-alloc-resource-to-client}. After allocating
the resources, continue on to the next step.
  
If you did not use a resource group, you will need to know the names
of all the resources defined in the previous step. If you forgot to
write them down, you should be able to figure the names out from a
listing of the \file{/tftpboot/lim} directory. A file located in the
directory, with the file's name being of the form
\file{resource\_name.resource\_type}, represents each resource defined
to LUI. To allocate the resources to your clients, enter the resource
names separated by commas in the \field{Resource Names} field and the
client group name in the \field{Machine Groups} field.  Be careful not
to use spaces in the resource names field, as they are not allowed.
  
If at any point after you allocate your resources to the client
machines you realize you need to change, add, or delete some
resources, you should use LUI to deallocate the resources. See the LUI
documentation for more information on deallocation of resources.
  
\begin{figure}[htbp]
  \begin{center}
    \includegraphics[scale=\imgscale]{allocate-resource.\figext}
    \caption{Allocate the Resource Group to the Client Group.}
    \label{fig:detailed-alloc-resource-to-client}
  \end{center}
\end{figure}

\subsection{Secondary OSCAR Server Configuration}
\label{sec:detailed-secondary-oscar-server-config}

During this phase, the server will be prepared for the client installs
based upon the information entered in the ``Cluster Definition''
phase.

\begin{enumerate}
\item Run secondary server configuration. Press the Step 6 button of
  the wizard entitled \button{Prepare for Client Installs}. This will
  run the pre client installation server configuration script, which
  does the following:

  \begin{enumerate}
  \item uses cluster information database to update \file{/etc/dhcpd.conf}
    
  \item installs C3 tools and man pages in \file{/opt/c3-2.7.2}

  \item generates OSCAR default resources and allocates them to clients
    
  \item configures NFS and ipchains services on server to allow OSCAR
    client installations

  \item makes sure DHCP server is running
  \end{enumerate}
  
\item Check for successful completion. In the output window for the
  above step, check for a message stating ``Begin booting client
  nodes'' before continuing. A sample successful output window is
  shown in Figure~\ref{fig:detailed-successful-output-window}. At this
  point, you may continue with the client installations phase. If you
  do not find this message in the output, try to find out what may
  have failed by searching through the output for error messages.
  
  If instead of the above message, you get a message stating that the
  ``DHCP server is not running'', see
  Appendix~\ref{app:troubleshooting-known-problems-dhcp}
  (page~\pageref{app:troubleshooting-known-problems-dhcp}) for help.
  Once you have the DHCP server running, you may continue with the
  client installations phase.
 
  \begin{figure}[htbp]
    \begin{center}
      \includegraphics[scale=\imgscale]{per-client.\figext}
      \caption{Successful Output Window for the ``Prepare for Client
        Installs'' Step.}
      \label{fig:detailed-successful-output-window}
    \end{center}
  \end{figure}
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Client Installations}

During this phase, you will network boot your client nodes and they
will automatically be installed and configured as specified in
Section~\ref{sec:detailed-cluster-def} above. For a detailed
explanation of what happens during client installation, see
Appendix~\ref{app:client-install}.

\subsubsection{Network boot the client nodes}

See Appendix~\ref{app:net-boot-client-nodes} for instructions on
network booting clients.

\subsubsection{Check completion status of nodes}

For each client, a log is kept detailing the progress of its
installation. The log files for all clients are kept on the server in
\file{/tftpboot/lim/log}. When a client installation completes, the
last line in the log for that client will read ``\msgout{installation
  is now complete, time to reboot!}'' Depending on the capabilities of
your server and the number of simultaneous client installations, a
client could take anywhere from five minutes to over an hour to
complete its installation.
  
\subsubsection{Reboot the client nodes}

After confirming that a client has completed its installation, you
should reboot the node from its hard drive. If you chose to have your
clients auto-reboot after installation, they will do this on their
own. If the clients are not set to auto-reboot, you must log in to the
node and reboot it. After logging in, issue the command ``\cmd{reboot
  -f}'', which issues a reboot with the force option. The force
option, which reboots the machine without shutting down any services,
is needed so that the node does not hang on shutdown of its network.
Since its file systems are network mounted during installation, the
attempt to shutdown the network will hang the machine if the force
option is not used. \msg{Note: If you had to change the BIOS boot
  order on the client to do a network boot before booting from the
  local disk, you will need to reset the order to prevent the node
  from trying to do another network install.}

\subsubsection{Check network connectivity to client nodes}

In order to perform the final cluster configuration, the server must
be able to communicate with the client nodes over the network. If a
client's ethernet adapter is not properly configured upon boot,
however, the server will not be able to communicate with the client. A
quick and easy way to confirm network connectivity is to do the
\begchange
following (assuming OSCAR installed in \file{/root}):

\begin{verbatim}
  cd /root/OSCAR-\oscarversion/scripts
  ./ping_clients
\end{verbatim}
\endchange

The above commands will run the \cmd{ping\_clients} script, which will
attempt to ping each defined client and will print a message stating
success or failure. If a client cannot be pinged, the initial ramdisk
provided probably did not have built in module support for its
ethernet adapter, and you will have to log in to the machine and
manually configure the adapter. Once all the clients have been
installed, rebooted, and their network connections have been
confirmed, you may proceed with the next step.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Cluster Configuration}

During this phase, the server and clients will be configured to work
together as a cluster.

\subsubsection{Complete the cluster configuration}

Press the Step 7 button of the wizard entitled \button{Complete
  Cluster Setup}.  This will run the \file{post\_install} script,
which does the following:

\begin{enumerate}
\item updates hosts files on server and clients
  
\item synchronizes password and group files on clients with server.
  
  Note that any users created on the server after the OSCAR
  installation will not be in the password/group files of the clients
  until they have been synced with the server - you can accomplish
  this using the C3 \cmd{cpush} tool.

\item updates clients' C3 configuration

\item configures user \cmd{rsh} capabilities

\item configures \cmd{ssh} on server and clients

  \begin{itemize}
  \item \user{root} authentication
  \item user authentication
  \end{itemize}

\item installs SystemImager on server and clients
  
\item installs and configures MPI-CH in \file{/usr/local/mpich-1.2.1}

\item installs and configures LAM/MPI in \file{/opt/lam-6.5.4}

\item installs and configures PVM in \file{/opt/pvm3}

\item installs Veridian's Portable Batch System (PBS)
  
  The PBS server and default scheduler are installed, but not the
  execution MOM, since the server machine is not used for computation.

\item installs the Maui scheduler for use with PBS

\item synchronizes the clients' date and time with the server
\end{enumerate}

\msg{Note: there will be a several minute delay before the output for
  this step will appear.}

\subsubsection{Check for successful completion}

In the output window for the above step, search for a message stating
``\msgout{Congratulations, your cluster is now ready for use.}'' A
sample successful output window is shown in
Figure~\ref{fig:detailed-successful-cluster-setup}. If you do not find
this message in the output, try to find out what may have failed by
searching through the output for error messages.

  \begin{figure}[htbp]
    \begin{center}
      \includegraphics[scale=\imgscale]{complete-cluster.\figext}
      \caption{Successful Output Window for the ``Complete
        Cluster Setup'' Step.}
      \label{fig:detailed-successful-cluster-setup}
    \end{center}
  \end{figure}


\subsubsection{Test your cluster using the OSCAR Cluster Test
  software}
            
Provided along with OSCAR is a simple test to make sure the key
cluster components (PBS, MPI, and PVM) are functioning properly. For
information on installing and running the software, see the
\file{oscar\_testing} document in the \file{docs} subdirectory.

