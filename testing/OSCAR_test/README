OSCAR Cluster Test
------------------

This testing suite *must* exist on a filesystem that is accessible on
all nodes in the cluster, such asn an NFS-exported /home directory.
Expanding this tarball in a local directory such as /tmp will cause
the MPI and PVM tests to fail.

The OSCAR cluster test is a simple script that runs several PBS jobs
in sequence in order to confirm that the cluster has been installed
and configured correctly. To do the test, just issue the following
command in the terminal:

   ./test_cluster [ <num_clients> <procs_per_client> ]

The number of clients and number of processors per client arguments
are optional. If not supplied, the script will prompt the user for the
values.

The first job runs the 'hostname' and 'date' programs on each of the
clients defined to PBS. This test is to make sure that PBS is
functioning correctly. If this test fails, the other two will not run
as they assume that PBS is working.

The next two jobs run the standard calculate pi (cpi) example program
provided with MPI on the clients.  This test is to make sure that
MPICH and LAM/MPI are functioning correctly. The 'pbs_script.mpich'
and 'pbs_script.lam' scripts may also be useful as a starting point
for generating your own PBS MPI jobs.

NOTE: The LAM script is a bit more complicated than is normally
necessary.  Since MPICH is the default MPI on OSCAR 1.1, the LAM PBS
script must set LAM/MPI to be the default, and then run the test.  See
the documentation for how to make LAM/MPI your default MPI and avoid
the extra complications in the pbs_script.lam script.

The third and final job runs the standard master-slave example program
provided with PVM on the clients. This test is to make sure that PVM
is functioning correctly. The 'pbs_script.pvm' script may also be
useful as a starting point for generating your own PBS PVM jobs.

As each job completes, the script will either show the contents of the
corresponding output file, or an error message indicating the job
failed. Assuming everything runs fine, your cluster should be ready
for production users. If you happen to get an error message indicating
job failure, check the file '<jobname>.err' in the jobs corresponding
subdirectory to try to find out what might have failed.

Sample 'test_cluster' output for a 2 client cluster:

#------------------Beginning of Output------------------
Running a simple PBS shell job...
Output from shell test:
node2.foobar.com
node1.foobar.com
Hello, date is 07/31/01, time is 14:16:23
Hello, date is 07/31/01, time is 14:14:31

Running a simple MPICH job under PBS...
Output from MPICH test:
Running MPICH test
Process 0 of 2 on node2.foobar.com
1000 iterations: pi is approx. 3.1415927370900434, error = 0.0000000835002503
wall clock time = 0.012759
Process 1 of 2 on node1.foobar.com


Running a simple LAM/MPI job under PBS...
Output from LAM/MPI test:
Running LAM/MPI test

LAM 6.5.4/MPI 2 C++/ROMIO - University of Notre Dame

Process 1 of 2 on node1.foobar.com
Process 0 of 2 on node2.foobar.com
1000 iterations: pi is approx. 3.1415927370900443, error = 0.0000000835002512
wall clock time = 0.000369

LAM 6.5.4/MPI 2 C++/ROMIO - University of Notre Dame



Running a simple PVM job under PBS...
Output from PVM test:
/tmp/fileciKsuV
Spawning 3 worker tasks ... SUCCESSFUL
I got 100.000000 from 1; (expecting 100.000000)
I got 200.000000 from 0; (expecting 200.000000)
I got 300.000000 from 2; (expecting 300.000000)
#--------------------End of Output-------------------
